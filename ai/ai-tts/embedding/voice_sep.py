# ìŒì„± íŒŒì¼ì—ì„œ íŠ¹ì • í™”ìì˜ ëª©ì†Œë¦¬ë§Œ ì¶”ì¶œí•˜ëŠ” ì½”ë“œ
from pyannote.audio import Pipeline
from pydub import AudioSegment
import os
from dotenv import load_dotenv
import torch

torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.environ.get("HF_TOKEN")

# GPU(CUDA) ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
if torch.cuda.is_available():
    device = "cuda"
    print("âœ… GPU(CUDA)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…í•©ë‹ˆë‹¤.")
else:
    device = "cpu"
    print("âš ï¸ GPU(CUDA)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…í•©ë‹ˆë‹¤.")

# Hugging Face ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
if not HF_TOKEN:
    print("ì˜¤ë¥˜: í™˜ê²½ ë³€ìˆ˜ 'HF_TOKEN'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

try:
    # íŒŒì´í”„ë¼ì¸ì„ íŠ¹ì • ì¥ì¹˜(GPU ë˜ëŠ” CPU)ë¡œ ë¡œë“œ
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization@2.1",
                                       use_auth_token=HF_TOKEN).to(torch.device(device))
except Exception as e:
    print(f"íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("Hugging Faceì—ì„œ 'pyannote/speaker-diarization' ë° 'pyannote/segmentation' ëª¨ë¸ì˜ ì•½ê´€ì— ë™ì˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

name = "joowoojae_real_wonbon"
# ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
input_file = f"sample_file/{name}.mp3"

# MP3 íŒŒì¼ì„ WAVë¡œ ë³€í™˜ (pyannoteëŠ” WAVë¥¼ ê¶Œì¥)
print("ğŸ”Š MP3 íŒŒì¼ì„ WAVë¡œ ë³€í™˜ ì¤‘...")
audio = AudioSegment.from_mp3(input_file)
wav_file = "temp_converted.wav"
audio.export(wav_file, format="wav")

# í™”ì ë¶„í•  ì‹¤í–‰
print("ğŸ™ï¸ í™”ì ë¶„í• (Diarization) ì‹œì‘...")
diarization = pipeline(wav_file)

# ì›í•˜ëŠ” í™”ì IDë¥¼ ì„¤ì •í•˜ì„¸ìš” (ì˜ˆ: SPEAKER_00, SPEAKER_01 ë“±)
target_speaker_id = "SPEAKER_01"

# íŠ¹ì • í™”ìì˜ ìŒì„±ë§Œ ë‹´ì„ ë¹ˆ AudioSegment ìƒì„±
target_speaker_audio = AudioSegment.empty()

# ë¶„í• ëœ ì˜¤ë””ì˜¤ êµ¬ê°„ë“¤ì„ í•©ì¹˜ê¸°
print(f"âœ‚ï¸ í™”ì({target_speaker_id})ì˜ ìŒì„± êµ¬ê°„ ì¶”ì¶œ ë° ë³‘í•© ì¤‘...")
full_audio = AudioSegment.from_file(wav_file)
for turn, _, speaker in diarization.itertracks(yield_label=True):
    if speaker == target_speaker_id:
        start_ms = turn.start * 1000
        end_ms = turn.end * 1000
        segment = full_audio[start_ms:end_ms]
        target_speaker_audio += segment

# ì¶”ì¶œëœ ìŒì„± íŒŒì¼ì„ WAVë¡œ ì €ì¥
output_file = f"extracted_audio/{target_speaker_id}_{name}1.wav"
os.makedirs("extracted_audio", exist_ok=True)
target_speaker_audio.export(output_file, format="wav")

# ì„ì‹œ íŒŒì¼ ì‚­ì œ
os.remove(wav_file)

print(f"âœ… íŠ¹ì • í™”ì({target_speaker_id})ì˜ ìŒì„± íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")